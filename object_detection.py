# imports
import os 
import argparse
import cv2
from random import randint
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model

# loading trained vgg model
model = load_model('weights/vgg.h5')

# function that returns squarred bounding boxe based on the original bounding box
def squared_boxes(x,y,w,h):
    size_diff = abs((w-h)//2)
    if w >= h :
        h = w 
        y = y - size_diff # center the box
    else : 
        w = h
        x = x - size_diff # center the box
    return (x,y,w,h)

# video and boxes directory 
videos_dir = 'VIDEOS/'
boxes_dir =  'GT/'

# default video displayed if no arguments are passed
index_video = randint(0,len(os.listdir(videos_dir))-1)
video_path = os.listdir(videos_dir)[index_video] 
gt_path = os.listdir(boxes_dir)[index_video]

# choose name and gt of displayed video
ap = argparse.ArgumentParser()
ap.add_argument("--video_path", default = videos_dir + str(video_path))
ap.add_argument("--gt_path", default = boxes_dir + str(gt_path))
video_path = ap.parse_args().video_path
gt_path = ap.parse_args().gt_path

# open txt file of boxes 
box_name = gt_path
box = open(box_name)

# class names dictionnary 
objects_dict = {0 : 'Bowl', 1 : 'CanOfCocaCola', 2 : 'MilkBottle', 3 : 'Rice', 4 : 'Sugar'}

# boxes dictionnary for each frame of the video
boxes = {}
for position, line in enumerate(box) : 
    if len(line.split()) == 6 :
        x = int(line.split()[2])
        y = int(line.split()[3])
        w = int(line.split()[4])
        h = int(line.split()[5])
        boxes[position] = (x, y, w, h)

# read video
position = 1 # first frame 
cap = cv2.VideoCapture(video_path)
while (cap.isOpened()):
    ret, frame = cap.read()
    if ret == False : 
        break 
    if position in boxes :
        # get position of object and draw bounding boxe
        
        # initial coordinates
        x, y , w, h = boxes[position]
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 1) 

        # coordinates generated by squared_boxes function 
        x0, y0, w0, h0 = squared_boxes(x, y, w, h)
        cv2.rectangle(frame, (x0, y0), (x0+w0, y0+h0), (0, 255, 0), 1)
    
        # cropping the region of interest (detected object) for each generated bounding box
        roi = frame[y:y + h, x:x + w]
        cropped_img =  np.expand_dims(np.expand_dims(cv2.resize(roi, (224, 224)), -1), 0)

        roi0 = frame[y0:y0 + h0, x0:x0 + w0]
        cropped_img0 =  np.expand_dims(np.expand_dims(cv2.resize(roi0, (224, 224)), -1), 0)

        # prediction on the detected object in the 4 cropped images
        prediction = model.predict(cropped_img)
        prediction0 = model.predict(cropped_img0)

        # we look for the maximum pourcentage of predicted object and save its maxindex
        maxvalue = max(prediction[0])
        maxindex = int(np.argmax(prediction))
        if max(prediction0[0]) > maxvalue : 
            maxindex = int(np.argmax(prediction0))

        cv2.putText(frame, objects_dict[maxindex], (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1, cv2.LINE_AA)

    cv2.imshow('frame', frame)
    if cv2.waitKey(100) & 0xFF == ord('q'):
        break 
    position += 1

cap.release()